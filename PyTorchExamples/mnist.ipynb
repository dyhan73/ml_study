{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4 * 4 * 50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "_StoreTrueAction(option_strings=['--save-model'], dest='save_model', nargs=0, const=True, default=False, type=None, choices=None, help='For Saving the current Model', metavar=None)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 13
    }
   ],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                    help='For Saving the current Model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-76ed0da66c72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unrecognized arguments: %s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1752\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1753\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2499\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2500\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'prog'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'message'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2501\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36mexit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2486\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2487\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2488\u001b[0;31m         \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2490\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2"
     ],
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error"
    },
    {
     "name": "stderr",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch-size N] [--test-batch-size N]\n                             [--epochs N] [--lr LR] [--momentum M] [--no-cuda]\n                             [--seed S] [--log-interval N] [--save-model]\nipykernel_launcher.py: error: unrecognized arguments: -f /Users/1001235/Library/Jupyter/runtime/kernel-48b618c3-c7b7-40a6-9fef-e38fc42b7701.json\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ],
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error"
    }
   ],
   "source": [
    "%tb\n",
    "#parser\n",
    "args = parser.parse_args()\n",
    "#args"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=1000, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "['0 - zero',\n '1 - one',\n '2 - two',\n '3 - three',\n '4 - four',\n '5 - five',\n '6 - six',\n '7 - seven',\n '8 - eight',\n '9 - nine']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 62
    }
   ],
   "source": [
    "train_loader.dataset.classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,  23, 141, 255, 227, 145,  17,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   2,\n          11,  89, 228, 254, 253, 253, 244,  45,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  40,\n         253, 253, 253, 213, 105,  89,  89,  26,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  41, 219,\n         254, 242, 112,  13,   0,   0,   2,  21,  21,  49,   4,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   9, 174, 253,\n         225,  34,   0,   0,   0,   0, 124, 253, 254, 199,   4,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  54, 253, 237,\n          62,   0,   0,   0,  32, 196, 251, 232, 157,  16,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 190, 253, 153,\n           0,   0,   2, 109, 243, 253, 212,  50,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  97, 253, 192,\n           0,   8,  84, 253, 253, 227,  30,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  91, 254, 190,\n           0, 164, 253, 254, 205,  20,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  26, 240, 253,\n         250, 253, 253, 242,  73,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 103, 253,\n         253, 253, 251, 150,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  78, 248,\n         254, 253, 149,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  77, 243, 253,\n         253, 253, 235,  49,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  78, 246, 253, 242,\n         183, 252, 253, 193,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0, 107, 244, 254, 220, 111,\n           0, 151, 254, 199,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0, 151, 253, 253, 195,  10,\n           0, 152, 253, 179,  12,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,  74, 238, 254, 238,  33,  22,\n         172, 253, 198,  45,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,  65, 254, 254, 246, 118, 194,\n         254, 236,  57,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   4, 196, 254, 254, 254, 254, 254,\n         254,  51,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   4, 200, 253, 253, 253, 181,  99,\n          54,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n       dtype=torch.uint8)\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x12d863da0>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 73
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_loader.dataset.data[545])\n",
    "plt.imshow(train_loader.dataset.data[545])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(8)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 64
    }
   ],
   "source": [
    "train_loader.dataset.targets.data[545]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.293206\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.398929\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.261139\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.244311\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.208471\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.124031\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.197761\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.189386\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.326726\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.051014\n",
      "\nTest set: Average loss: 0.0807, Accuracy: 9757/10000 (98%)\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "train(model, device, train_loader, optimizer, 1)\n",
    "test(model, device, test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.031348\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.101865\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.140083\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.047237\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.015814\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.103996\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.029551\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.053649\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.155893\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.069987\n",
      "\nTest set: Average loss: 0.0658, Accuracy: 9801/10000 (98%)\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "train(model, device, train_loader, optimizer, 2)\n",
    "test(model, device, test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.092686\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.027803\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.150815\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.040926\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.042622\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.136419\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.051782\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.037577\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.050735\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.022587\n",
      "\nTest set: Average loss: 0.0483, Accuracy: 9841/10000 (98%)\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "train(model, device, train_loader, optimizer, 3)\n",
    "test(model, device, test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.046789\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.022696\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.079017\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.097384\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.040312\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.128733\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.006495\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.031472\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.093547\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.028888\n",
      "\nTest set: Average loss: 0.0381, Accuracy: 9873/10000 (99%)\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "train(model, device, train_loader, optimizer, 4)\n",
    "test(model, device, test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.016209\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.009542\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.077293\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.058302\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.022176\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.012908\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.094640\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.015083\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.049532\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.013540\n",
      "\nTest set: Average loss: 0.0371, Accuracy: 9886/10000 (99%)\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "train(model, device, train_loader, optimizer, 5)\n",
    "test(model, device, test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.weight',\n              tensor([[[[ 9.9038e-02,  1.3467e-01,  1.3211e-01,  1.9545e-01, -3.6098e-02],\n                        [-1.1920e-01,  4.1907e-02,  1.0993e-01,  1.7918e-02,  6.1213e-02],\n                        [ 2.1201e-01, -2.4438e-02,  2.4877e-02, -2.1449e-01, -1.9199e-02],\n                        [ 2.1653e-01,  2.4317e-01,  1.1764e-01, -5.0203e-02, -2.5267e-01],\n                        [ 2.4052e-01,  6.5323e-03, -1.4007e-01, -9.8759e-02, -1.9873e-01]]],\n              \n              \n                      [[[ 4.9599e-02,  1.0361e-01,  1.0746e-01, -4.2895e-02, -8.1431e-02],\n                        [-1.8011e-01, -2.4550e-01, -2.6418e-01, -9.0812e-02, -1.4901e-01],\n                        [-4.0589e-02, -1.1692e-01,  4.1930e-02, -1.4445e-01,  2.1746e-01],\n                        [ 1.0862e-01,  3.0814e-02,  1.8583e-02,  1.3235e-01,  2.7302e-01],\n                        [ 2.5942e-01,  3.8325e-01,  2.5617e-01,  3.3334e-01,  9.5447e-02]]],\n              \n              \n                      [[[-1.2871e-02,  2.1636e-02,  3.1010e-02, -7.0196e-02, -8.8101e-02],\n                        [ 5.2155e-02,  1.2079e-01, -1.6958e-01, -1.5864e-01, -1.9683e-01],\n                        [-1.8255e-01, -2.1004e-02,  1.6808e-01, -1.1578e-01, -1.9311e-01],\n                        [ 2.0852e-02, -1.6601e-01, -2.1213e-01, -1.1538e-01,  3.2404e-02],\n                        [ 7.1984e-02, -1.7606e-01,  1.5594e-01, -1.7825e-01, -4.3587e-02]]],\n              \n              \n                      [[[ 8.7409e-02, -6.4515e-02,  1.9199e-02,  1.1796e-01, -6.2826e-03],\n                        [ 5.2370e-02,  1.2845e-01,  1.2947e-01,  2.5198e-02, -2.2944e-01],\n                        [ 1.7655e-01,  5.4097e-03, -4.0390e-03, -1.2928e-01, -1.9135e-01],\n                        [ 2.4851e-01,  5.2479e-02,  1.1631e-01, -1.0676e-01, -2.1733e-01],\n                        [ 1.5276e-01, -1.4943e-02, -1.3571e-02, -1.1236e-01, -2.3156e-01]]],\n              \n              \n                      [[[-1.1745e-01,  2.1843e-02, -7.5500e-02, -1.0102e-01,  1.6314e-01],\n                        [ 3.0863e-02,  2.6088e-02,  1.7656e-01, -6.2737e-02,  8.8040e-02],\n                        [ 1.6292e-01,  1.7511e-01,  3.2015e-01,  6.0051e-02,  8.1789e-02],\n                        [-1.3925e-01,  2.5354e-01,  3.0800e-01, -3.1265e-02, -1.9172e-01],\n                        [ 1.9033e-02,  5.4271e-02, -1.1706e-02, -1.2510e-02, -4.7804e-02]]],\n              \n              \n                      [[[ 9.0091e-02,  4.9086e-02, -2.2300e-05,  1.4684e-01,  6.4980e-02],\n                        [-9.7581e-02,  3.4970e-02,  2.6643e-01,  1.5534e-01, -2.2637e-01],\n                        [ 1.7268e-01,  3.4170e-01,  2.8755e-01, -7.9015e-02, -2.2824e-01],\n                        [ 2.1030e-01,  2.1959e-01, -4.6031e-02, -1.5834e-01, -1.9735e-01],\n                        [ 2.3057e-01,  3.1242e-01,  5.8626e-02, -2.1331e-01, -5.9110e-02]]],\n              \n              \n                      [[[-2.1039e-01, -1.2557e-01, -1.3407e-03, -2.2486e-02,  2.5463e-01],\n                        [ 1.0010e-01, -2.1598e-01, -1.0041e-02,  4.4699e-02,  2.6949e-01],\n                        [-2.4325e-01, -3.8563e-02,  1.0056e-01,  1.1919e-01,  2.5698e-02],\n                        [-1.7894e-01, -1.1369e-01,  1.7469e-01,  1.9850e-01,  1.9134e-01],\n                        [ 1.9263e-02, -1.7099e-02,  1.7142e-01,  2.3336e-01, -9.9784e-02]]],\n              \n              \n                      [[[-1.4563e-01,  1.7577e-01,  4.9636e-02,  2.8890e-01,  1.6491e-01],\n                        [ 1.3766e-01,  3.5752e-01,  2.9242e-01,  2.2157e-01,  3.9506e-02],\n                        [ 7.5955e-02,  1.4901e-01,  9.2790e-02, -2.5104e-03, -6.4014e-02],\n                        [ 1.0503e-01, -1.9126e-01, -2.1222e-01, -2.4630e-02, -5.1582e-02],\n                        [-1.4916e-01, -1.8337e-02, -1.2580e-01, -2.0082e-02,  2.3826e-01]]],\n              \n              \n                      [[[ 1.0370e-02, -1.7193e-01,  3.5885e-02,  6.1280e-02,  2.1084e-01],\n                        [-6.2413e-02,  1.0859e-01,  8.4242e-02,  2.3338e-01,  1.2026e-01],\n                        [-2.2883e-01,  7.3492e-02,  2.5910e-01,  2.4673e-01,  2.4567e-01],\n                        [-2.2778e-01, -1.1080e-01, -5.9941e-02,  3.5435e-01, -1.3236e-01],\n                        [-1.6218e-01,  1.4377e-01,  2.5407e-01,  2.2560e-01, -8.3779e-02]]],\n              \n              \n                      [[[ 3.2584e-02,  1.7332e-01, -1.2516e-02, -1.3403e-01,  1.0202e-01],\n                        [ 1.9339e-01,  6.9315e-02, -6.8480e-02,  2.3983e-01, -6.3657e-02],\n                        [ 1.7794e-03, -9.3088e-02,  9.4293e-03, -5.6818e-02,  2.3499e-01],\n                        [-6.3881e-02,  1.1011e-01, -1.5134e-01,  4.4794e-03, -1.7315e-01],\n                        [ 3.7760e-02, -1.6508e-01,  4.0396e-02, -5.6784e-02,  1.7613e-01]]],\n              \n              \n                      [[[-1.2373e-01, -1.0015e-01, -1.4287e-01, -3.4691e-02,  1.3705e-01],\n                        [-4.3739e-02, -2.2875e-01, -4.7940e-02, -2.3704e-01, -2.5981e-01],\n                        [ 9.5901e-02, -1.5312e-02, -3.6913e-02, -4.1549e-02, -1.0575e-01],\n                        [-1.2744e-01,  1.4592e-01,  2.5105e-01, -9.4480e-02,  1.6824e-01],\n                        [ 2.1385e-01,  1.9129e-01,  1.7706e-02,  1.3829e-01,  2.9181e-01]]],\n              \n              \n                      [[[ 3.5777e-02,  1.5792e-01,  3.2324e-02,  7.6150e-03, -9.1209e-02],\n                        [ 1.4054e-01, -5.6053e-02, -1.3361e-01, -1.7072e-01, -1.2342e-01],\n                        [ 9.8927e-02,  2.0742e-01,  1.0047e-01, -5.8795e-02, -2.1092e-02],\n                        [ 6.4884e-02, -1.3977e-01, -3.7434e-02, -1.9108e-01, -1.1433e-01],\n                        [ 2.4390e-01,  4.9055e-03,  1.7988e-01, -3.0436e-03,  2.3152e-01]]],\n              \n              \n                      [[[-6.9675e-02, -6.4812e-02, -2.1701e-01, -2.0697e-02, -6.0706e-02],\n                        [ 1.1545e-01,  2.0899e-01,  2.0791e-01, -1.1951e-01, -2.1973e-01],\n                        [ 1.3056e-01,  2.2451e-01,  3.3853e-01,  3.7312e-01,  2.1017e-01],\n                        [ 2.8700e-01, -2.5991e-02,  3.7624e-01,  3.4383e-01,  2.7603e-01],\n                        [ 1.9723e-01,  1.3395e-01,  1.2072e-01, -8.1327e-02, -1.4553e-01]]],\n              \n              \n                      [[[ 9.1130e-02, -2.9244e-02, -8.3206e-02,  1.9323e-01,  1.7230e-01],\n                        [-9.6634e-02, -1.2400e-02,  1.3637e-01,  1.0439e-02, -2.0656e-02],\n                        [ 1.7548e-01, -4.2989e-02, -1.1270e-01,  5.7197e-02, -7.7020e-04],\n                        [-1.4807e-01, -6.8936e-03, -5.5392e-02,  1.9443e-01, -1.6101e-01],\n                        [-1.8300e-01, -2.0144e-01,  4.8628e-02,  4.4551e-02,  5.6326e-02]]],\n              \n              \n                      [[[ 2.1643e-01,  1.1289e-01, -2.5458e-01, -2.1782e-01, -2.1458e-01],\n                        [-7.6405e-03,  1.0105e-01, -1.8148e-01, -5.6737e-02, -5.3148e-02],\n                        [ 1.6560e-01,  1.2449e-01,  9.8805e-02, -2.3745e-01, -2.4157e-01],\n                        [ 2.8534e-01, -1.6735e-02,  2.6884e-01, -1.6880e-02,  8.4338e-02],\n                        [ 2.3653e-01,  3.1455e-01,  1.5929e-01, -1.7083e-02,  1.2340e-01]]],\n              \n              \n                      [[[-1.1742e-01, -1.2899e-01,  1.3496e-01,  2.2416e-01, -1.2554e-01],\n                        [-4.3976e-02, -1.2906e-01,  2.5273e-01,  2.3465e-01,  1.2033e-01],\n                        [-1.8382e-01, -1.2511e-01,  1.2731e-02,  2.6686e-01, -1.0702e-02],\n                        [-1.6587e-01, -2.1212e-01,  7.7816e-02,  1.9662e-01, -1.0774e-01],\n                        [-1.6827e-01, -2.1339e-01, -1.1047e-01, -6.7565e-02, -8.2640e-02]]],\n              \n              \n                      [[[-1.3894e-01, -6.1666e-02,  3.1798e-01,  2.4022e-01,  7.4315e-02],\n                        [ 8.9671e-02,  1.8335e-01,  1.6108e-01,  1.7199e-01, -7.9533e-02],\n                        [-7.2625e-02,  2.9959e-01,  2.2509e-01,  2.1526e-01,  1.0211e-01],\n                        [ 2.0448e-01,  2.1764e-01,  1.4442e-01,  2.8262e-01,  1.6373e-01],\n                        [-1.7276e-01,  2.1624e-01,  1.9692e-01,  5.0660e-02, -8.1681e-03]]],\n              \n              \n                      [[[ 1.3732e-01, -1.1551e-01,  3.5046e-02, -1.5555e-01, -2.4335e-01],\n                        [-8.4149e-02,  6.6721e-02, -2.5480e-01, -2.9101e-01, -2.6076e-01],\n                        [ 1.2250e-01,  1.0263e-01,  2.9570e-01,  5.2717e-02, -1.6481e-04],\n                        [ 8.2169e-02,  2.3189e-01,  3.2788e-01,  4.2718e-01,  2.9181e-01],\n                        [ 1.3860e-01, -1.0854e-01, -5.9658e-02,  1.2152e-01,  2.6821e-01]]],\n              \n              \n                      [[[ 2.2596e-01, -1.8035e-02, -7.1431e-02,  3.4966e-03, -1.0312e-01],\n                        [-8.3725e-02, -1.2631e-01, -1.1109e-01, -2.0931e-01,  1.9967e-02],\n                        [-7.5313e-02, -1.2090e-01,  1.7165e-02, -1.2182e-01, -9.3723e-02],\n                        [ 5.2721e-02, -7.9869e-03, -1.7330e-01,  8.6477e-02,  1.1254e-01],\n                        [-1.9196e-01, -1.9887e-01, -4.1250e-02, -1.7459e-01,  1.7435e-01]]],\n              \n              \n                      [[[ 8.9332e-02, -1.9214e-01, -1.5576e-01,  1.2045e-01, -1.7370e-01],\n                        [-1.6945e-01,  4.8956e-02,  8.7122e-02,  2.9047e-01,  2.8427e-01],\n                        [ 9.8051e-02, -6.0973e-03, -9.9288e-02,  1.9065e-01,  2.1950e-01],\n                        [ 2.6125e-02,  2.6037e-01,  6.2492e-03, -9.6019e-03,  1.9082e-01],\n                        [ 1.9726e-01,  1.2990e-01,  5.8927e-02, -2.9917e-02, -1.2190e-01]]]])),\n             ('conv1.bias',\n              tensor([-0.1436,  0.0182,  0.0481,  0.1618,  0.1852, -0.1790, -0.1042,  0.0747,\n                       0.0315, -0.0598, -0.0265, -0.0687,  0.1067,  0.1620, -0.0686, -0.1518,\n                       0.1822,  0.2516, -0.0828,  0.0800])),\n             ('conv2.weight',\n              tensor([[[[ 2.2575e-02,  3.5408e-02,  6.9722e-03, -2.0026e-02, -7.8280e-03],\n                        [ 2.0786e-02,  1.4092e-02,  3.4072e-02, -1.5385e-02, -3.8534e-02],\n                        [-2.7122e-02, -2.8885e-02, -7.8633e-03,  1.2505e-02, -4.9621e-03],\n                        [ 1.0301e-02, -1.5473e-03,  4.2909e-03,  4.0510e-02, -2.6003e-02],\n                        [-4.4487e-02,  6.6118e-03,  6.0954e-03, -1.1030e-02, -4.7826e-03]],\n              \n                       [[ 7.2901e-03,  2.9108e-02, -5.2450e-02, -5.2895e-03,  1.6900e-02],\n                        [ 1.5076e-02,  4.4311e-02, -1.5028e-02, -3.2988e-02, -4.2453e-02],\n                        [-4.2957e-02,  1.8715e-02, -3.9500e-02, -4.2518e-02,  2.6454e-03],\n                        [ 1.7960e-02,  1.9025e-03, -4.0742e-02,  8.1154e-03, -5.1349e-02],\n                        [-2.7915e-02, -2.4255e-02, -1.9132e-02,  1.3406e-02, -4.6859e-02]],\n              \n                       [[ 2.6843e-02,  9.8621e-03,  3.5588e-02,  1.0080e-02, -4.4031e-02],\n                        [-2.8804e-02,  3.2531e-02,  1.5749e-04, -2.5202e-02, -3.6877e-02],\n                        [ 1.3677e-02,  3.0188e-02,  3.1993e-03,  2.5652e-02,  3.3447e-02],\n                        [-4.3096e-02, -1.2237e-02,  3.0463e-02,  1.1734e-02,  2.0991e-02],\n                        [ 3.7004e-02,  4.3977e-02, -9.8307e-04, -3.9204e-02,  1.5596e-02]],\n              \n                       ...,\n              \n                       [[-7.2241e-03,  1.9648e-04,  8.9353e-03, -2.2483e-02, -4.6868e-02],\n                        [ 2.2496e-02,  2.0594e-03,  5.2016e-02,  3.1078e-02, -2.1106e-02],\n                        [ 1.2282e-02,  1.9856e-02,  1.7001e-02,  3.5499e-03, -4.2288e-02],\n                        [-6.4709e-03, -3.3130e-03, -2.3558e-02,  3.5262e-02,  1.2834e-02],\n                        [-4.6582e-02,  3.1571e-02,  7.6472e-03, -3.3393e-03, -1.2068e-03]],\n              \n                       [[-1.7495e-02,  3.7310e-02, -3.9057e-03, -4.3052e-02,  4.5301e-02],\n                        [ 3.7350e-03, -3.2434e-02, -1.4148e-02,  1.5852e-02,  4.6216e-02],\n                        [-3.8312e-02, -2.7412e-02, -4.9663e-02, -3.0625e-02,  3.8677e-02],\n                        [ 3.6663e-02, -4.6972e-02, -2.6107e-02, -1.6554e-02,  1.1564e-02],\n                        [-1.6967e-02, -2.1911e-03,  2.8875e-03,  4.2014e-02,  4.5601e-02]],\n              \n                       [[ 3.5332e-02, -1.5558e-02,  1.9307e-02, -4.7178e-02,  3.5386e-02],\n                        [-1.0582e-02, -4.5049e-02, -4.0669e-02, -3.1377e-02, -2.8694e-03],\n                        [ 2.9494e-02, -3.3639e-02,  3.2707e-02,  2.4334e-02, -1.1790e-02],\n                        [-2.7089e-03,  1.1027e-03, -4.2108e-03,  3.7020e-02, -2.0654e-02],\n                        [ 1.1970e-02, -1.7038e-02,  3.4553e-02, -1.9849e-02, -2.4708e-03]]],\n              \n              \n                      [[[ 1.1367e-02,  2.1500e-02,  3.3683e-02,  2.0763e-02,  1.0765e-02],\n                        [ 3.5085e-02, -4.3460e-03,  1.2195e-02, -4.5593e-02, -1.9294e-02],\n                        [-1.9065e-02, -4.6316e-02,  4.8110e-02,  4.4884e-02,  4.2014e-02],\n                        [ 3.1659e-02, -1.9277e-02, -4.0212e-02, -2.4346e-02, -4.9101e-03],\n                        [ 1.1950e-02, -3.1663e-02,  3.8363e-02, -3.5743e-02,  3.3982e-02]],\n              \n                       [[ 1.6077e-02,  4.6311e-02,  4.7111e-02,  5.1553e-03, -2.4244e-02],\n                        [ 4.1453e-02,  3.9662e-02,  8.0212e-03, -3.5754e-02, -4.8923e-02],\n                        [ 6.0116e-03, -2.0510e-02,  6.3030e-02,  3.2136e-02,  3.8736e-03],\n                        [ 3.6511e-02,  4.4557e-02,  3.0010e-02,  1.5244e-02, -1.7638e-02],\n                        [-3.8994e-02,  1.7806e-02, -9.9279e-03, -3.0958e-02,  7.0333e-03]],\n              \n                       [[ 1.9417e-02, -2.9419e-02,  2.7609e-02, -1.0783e-02,  2.7621e-02],\n                        [ 9.4880e-04, -3.6909e-02, -3.2271e-02, -6.5523e-03, -2.8388e-02],\n                        [-2.9615e-02,  9.9039e-03, -2.8391e-02,  1.0421e-02,  4.4977e-02],\n                        [-1.1982e-02,  1.1318e-02, -2.3332e-02, -3.6637e-04,  3.9769e-02],\n                        [ 3.3311e-02,  4.3401e-03, -4.2831e-02,  1.1923e-02, -3.1230e-02]],\n              \n                       ...,\n              \n                       [[ 4.8364e-02, -2.8025e-02,  3.7840e-02,  6.0273e-03,  2.9397e-02],\n                        [ 8.0375e-02,  3.7969e-02, -1.3366e-02, -9.0232e-03, -3.8305e-02],\n                        [ 6.4725e-02,  4.2978e-02,  5.9414e-02,  2.0235e-03, -3.3168e-02],\n                        [ 3.6440e-02,  8.9966e-02,  8.9302e-02,  4.7337e-02,  1.9024e-02],\n                        [-3.0983e-02, -3.9477e-02,  3.7624e-02,  5.1379e-02, -3.0247e-02]],\n              \n                       [[-2.3116e-02, -3.7732e-02, -1.0848e-02, -1.6994e-02, -2.7120e-02],\n                        [ 3.0040e-02,  5.8267e-03,  1.8528e-02,  3.0513e-02,  1.0358e-04],\n                        [-3.8685e-02, -2.4264e-02, -2.3958e-02, -1.1681e-02, -3.0308e-02],\n                        [-4.9292e-03, -4.7328e-02, -3.7412e-02,  1.7788e-02, -3.4309e-02],\n                        [-4.4313e-02,  8.8578e-03,  1.0279e-02, -3.8495e-04, -1.8926e-02]],\n              \n                       [[ 3.4178e-03,  5.0047e-02,  5.1573e-03,  4.1558e-02, -2.8415e-02],\n                        [ 1.4928e-02,  3.6352e-02, -3.1354e-02,  1.3548e-02, -5.3314e-02],\n                        [-6.9224e-03,  1.4670e-02,  4.9409e-03,  4.0174e-02,  2.7477e-03],\n                        [ 7.4946e-03,  2.6068e-02, -1.1179e-02,  5.6775e-05,  1.3071e-02],\n                        [-5.5151e-05, -8.0254e-03, -2.8583e-02, -1.5338e-02, -5.6566e-03]]],\n              \n              \n                      [[[-3.6968e-02,  3.8270e-03, -1.0756e-02,  2.6514e-02,  4.3256e-03],\n                        [-3.2535e-03, -9.1587e-03, -3.8840e-02,  1.6151e-02, -2.6849e-02],\n                        [ 1.6364e-02,  1.2280e-02, -4.7457e-02,  2.4038e-02,  3.5701e-02],\n                        [-2.5842e-02,  4.0278e-02, -3.4290e-02, -2.0048e-02, -3.9745e-02],\n                        [-3.6430e-03, -3.1535e-02,  2.5044e-03,  3.3871e-02, -2.7989e-02]],\n              \n                       [[ 3.0066e-02,  4.2383e-02,  4.9883e-02,  9.6642e-03, -3.9811e-02],\n                        [ 2.6223e-02, -3.9670e-02, -2.6105e-02, -1.0891e-02, -3.1180e-02],\n                        [-9.2313e-03, -1.1242e-02, -1.8121e-02,  1.9132e-02,  2.8789e-02],\n                        [ 3.6251e-02, -4.4297e-02, -1.7631e-02,  3.2862e-02,  3.2391e-02],\n                        [ 2.9569e-02,  2.3721e-02, -5.2552e-02,  2.4324e-02, -5.4785e-03]],\n              \n                       [[ 2.7398e-02, -2.4791e-03,  2.4000e-02,  3.5885e-02,  9.8554e-03],\n                        [-3.2056e-02, -3.9171e-02,  7.3115e-03,  5.4186e-03,  1.6738e-02],\n                        [ 4.1220e-02,  1.2764e-02,  6.1133e-03, -2.7463e-02,  1.8864e-02],\n                        [-4.0728e-02, -3.8076e-02,  3.2781e-02,  1.6840e-02, -3.5515e-02],\n                        [ 1.4712e-02, -2.7711e-02, -1.1245e-02,  5.1562e-03,  7.9454e-03]],\n              \n                       ...,\n              \n                       [[ 1.2795e-02, -2.7040e-02,  5.7712e-03, -5.5249e-04, -2.5223e-02],\n                        [-3.0683e-03,  1.8651e-02,  2.6204e-03, -3.0512e-02, -1.1326e-02],\n                        [-3.4736e-03,  8.2079e-03, -5.5715e-03, -5.4957e-03,  2.5940e-02],\n                        [ 3.3020e-02, -4.5875e-02, -2.0758e-02,  3.4301e-02, -2.1655e-02],\n                        [ 8.0181e-03, -3.9540e-02, -3.3102e-02, -3.0488e-02,  6.1409e-03]],\n              \n                       [[-2.6746e-02,  3.4953e-02, -2.4582e-02, -3.7786e-02,  1.7623e-02],\n                        [ 9.7958e-04, -9.6894e-03, -6.2309e-03,  3.3453e-02,  5.3075e-02],\n                        [-1.0006e-02, -2.0349e-02, -5.7207e-03,  2.2937e-02,  1.8350e-03],\n                        [ 4.2352e-02,  3.3080e-02, -1.3598e-02, -2.4668e-02, -2.3787e-02],\n                        [-3.3727e-02,  2.6791e-02, -1.2785e-02, -9.6163e-03,  2.7945e-02]],\n              \n                       [[ 2.1839e-02, -2.6904e-02,  2.1192e-02,  3.3217e-02,  3.9517e-03],\n                        [ 1.3437e-02, -1.5093e-02, -4.3374e-02, -5.3871e-02, -4.2077e-02],\n                        [-4.2184e-02,  8.2128e-03, -2.0256e-02, -3.5061e-04, -2.4635e-02],\n                        [ 2.2053e-02, -3.6778e-02, -1.7639e-02,  2.8933e-02, -1.1668e-02],\n                        [-6.8374e-03, -3.8489e-02, -2.3331e-02,  2.4301e-02, -2.8424e-02]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 3.6059e-03, -2.7282e-03, -5.0702e-03, -2.0256e-02,  2.0683e-02],\n                        [-6.7175e-03, -1.2167e-02,  5.0882e-02,  1.2221e-02,  3.9441e-02],\n                        [ 3.6264e-02, -2.5435e-02, -1.9208e-02,  1.4522e-02, -2.6275e-02],\n                        [ 3.4482e-02, -9.0843e-03, -4.0523e-02,  2.9194e-02,  3.0142e-02],\n                        [ 7.8064e-03, -1.2814e-02,  1.2651e-02, -3.8358e-02, -3.8347e-02]],\n              \n                       [[-1.6916e-02,  3.5288e-02, -3.3432e-03,  1.3124e-02,  5.0899e-02],\n                        [ 2.7206e-02,  3.9791e-02,  2.3446e-02,  5.0041e-03,  4.9831e-03],\n                        [-1.1603e-02,  2.6463e-02,  2.5365e-02,  4.4623e-02,  3.8300e-02],\n                        [-2.7891e-02, -6.0352e-02, -4.7055e-03, -9.6338e-03,  2.5366e-02],\n                        [ 3.5743e-02,  2.3294e-02, -9.3697e-03, -3.8766e-02, -2.9317e-02]],\n              \n                       [[-2.6136e-02, -2.8724e-03, -1.9088e-02,  3.6630e-02,  1.2284e-02],\n                        [-4.5078e-02,  2.3182e-02,  9.0509e-03,  9.7706e-04,  1.5452e-02],\n                        [-2.8015e-02,  8.9678e-03,  5.1015e-03,  2.8612e-02,  1.6344e-02],\n                        [-2.0599e-03, -2.8729e-02, -3.5906e-02,  2.0167e-02, -1.0583e-02],\n                        [-1.3507e-02,  2.0422e-02,  2.9384e-02, -3.0941e-02,  1.2950e-02]],\n              \n                       ...,\n              \n                       [[ 7.5540e-02,  6.5452e-02,  3.2124e-02,  2.0287e-02, -2.5251e-02],\n                        [ 6.5210e-02,  9.5830e-02,  7.7844e-03,  2.3880e-02, -1.6036e-04],\n                        [-4.9293e-02,  4.0747e-02,  1.3378e-01,  6.9106e-02, -2.8053e-02],\n                        [-3.7583e-02, -7.2161e-03,  7.4443e-02,  7.4203e-02, -8.0790e-03],\n                        [-1.2939e-02, -5.3295e-02, -6.3651e-02,  2.1899e-02, -1.2806e-02]],\n              \n                       [[-3.5211e-02,  2.7949e-02,  8.9487e-03, -4.0854e-02,  2.6201e-02],\n                        [-1.1895e-02,  3.1886e-03, -3.0844e-02, -6.3603e-03,  1.6668e-03],\n                        [-2.7267e-03,  5.2749e-03, -4.8136e-02,  1.0122e-02, -1.6028e-02],\n                        [-9.6523e-04, -2.7088e-02, -1.7721e-02, -2.3139e-02,  2.0372e-02],\n                        [-3.9433e-02,  2.0376e-02, -4.2304e-02, -2.9982e-02,  2.9251e-02]],\n              \n                       [[ 2.2411e-03, -2.8868e-02, -2.9448e-02,  4.3285e-02, -2.4506e-02],\n                        [ 1.2165e-02, -4.0165e-02,  3.0656e-03, -7.7872e-03, -1.8063e-02],\n                        [-2.7480e-02,  5.2387e-02, -2.0437e-02,  5.6303e-02,  8.8331e-03],\n                        [-2.7196e-02, -6.1423e-03,  5.7700e-02,  1.7318e-02,  4.1735e-02],\n                        [ 7.7398e-03,  1.9434e-02, -1.4250e-02, -2.9417e-02,  1.4282e-04]]],\n              \n              \n                      [[[-4.6884e-02,  2.1621e-02,  3.0189e-02, -8.3524e-03,  6.4115e-03],\n                        [ 5.7854e-03, -2.5792e-02, -4.2435e-02,  4.7394e-02,  6.8285e-03],\n                        [ 3.8989e-02, -5.2484e-02,  1.5511e-02,  4.9148e-02,  4.5892e-02],\n                        [ 3.5917e-02,  1.1780e-02,  3.5417e-03,  1.3744e-03,  2.9976e-02],\n                        [ 2.8590e-02,  3.9321e-02,  1.5734e-02,  6.6583e-03,  1.2166e-02]],\n              \n                       [[ 2.1719e-02,  1.1301e-02,  3.0618e-02, -4.9708e-02,  2.1293e-02],\n                        [-2.5118e-02,  2.8353e-02, -7.6336e-03, -1.3703e-02,  2.1664e-02],\n                        [ 3.7746e-02,  6.4494e-02,  6.4644e-02,  2.2042e-02, -4.7454e-02],\n                        [-1.0732e-02,  6.5575e-02,  4.1738e-02,  2.8161e-03,  2.2532e-02],\n                        [-9.5379e-03,  1.0820e-02,  1.2440e-02, -4.8106e-02, -4.8998e-02]],\n              \n                       [[ 1.8754e-02,  3.2454e-02, -9.6578e-03,  9.1243e-03, -3.7964e-03],\n                        [ 2.1136e-02, -4.5248e-02,  1.8941e-04,  3.2588e-02, -4.8012e-02],\n                        [ 1.8511e-02,  3.2478e-02,  1.3277e-02,  1.9858e-02,  2.0469e-03],\n                        [-2.1214e-02,  8.3138e-03, -4.4780e-02, -5.4779e-03, -4.5948e-02],\n                        [-3.2643e-02,  5.0307e-03, -4.5728e-02, -4.5387e-02, -2.3998e-02]],\n              \n                       ...,\n              \n                       [[ 2.5590e-02, -6.0907e-03, -2.4489e-02, -3.7580e-02, -3.8098e-02],\n                        [-1.1347e-02, -1.4151e-02,  3.3619e-02, -8.9077e-03, -1.7000e-03],\n                        [ 4.3927e-03,  3.0599e-02, -4.9374e-03, -4.9508e-02, -3.4904e-02],\n                        [-1.1554e-02,  2.3320e-02, -2.5161e-02, -5.7803e-02, -1.0306e-02],\n                        [ 1.8174e-02, -1.5298e-02, -4.0508e-02, -3.4098e-02, -1.7481e-03]],\n              \n                       [[-2.5065e-02,  1.2755e-02,  1.4237e-02, -5.3431e-03, -1.2085e-02],\n                        [ 1.8504e-03,  1.7145e-03, -9.6201e-03, -2.0813e-02,  2.5123e-02],\n                        [-4.8073e-02,  7.7032e-04,  3.4476e-02,  3.4950e-02, -1.3670e-04],\n                        [-2.1206e-02, -1.3816e-02,  2.9248e-02, -3.1705e-02, -3.0118e-02],\n                        [ 2.0480e-02, -3.8421e-04, -3.0073e-03,  3.0275e-02,  1.3133e-02]],\n              \n                       [[ 2.2180e-02, -3.8832e-03, -3.0542e-02,  2.3555e-02, -7.1596e-03],\n                        [ 4.5742e-02,  3.6280e-02, -3.1380e-02, -2.4874e-02,  5.3423e-02],\n                        [-3.7494e-02,  4.3638e-02,  1.0761e-02,  4.2939e-02,  4.1541e-02],\n                        [ 3.1693e-02,  3.1796e-02,  4.2605e-02,  3.1623e-02, -1.3013e-02],\n                        [ 1.4780e-02,  4.5716e-02,  1.7626e-02, -3.8733e-02, -4.5739e-03]]],\n              \n              \n                      [[[-4.7062e-02, -1.3693e-02,  2.4610e-02, -2.4320e-02, -3.4873e-02],\n                        [ 2.9786e-02, -2.3015e-02, -5.3219e-02,  1.1465e-02, -2.0638e-02],\n                        [ 2.2377e-02, -3.2396e-02,  3.0058e-02, -2.0030e-02, -4.0403e-02],\n                        [ 3.3597e-02, -1.9260e-02,  3.6319e-02,  4.4174e-02,  2.3377e-02],\n                        [ 3.2863e-02, -2.3317e-02,  4.3900e-02,  3.3976e-03,  3.0421e-02]],\n              \n                       [[-4.9148e-02, -4.6213e-02, -3.2291e-02, -1.4713e-02, -6.4447e-03],\n                        [-4.8154e-02,  1.1912e-02,  2.1772e-02, -1.3662e-03,  1.2050e-02],\n                        [ 4.9178e-02,  7.2606e-02,  7.1190e-02,  3.8585e-02,  3.2537e-02],\n                        [-1.5823e-03,  4.6498e-02,  4.2286e-02, -2.2013e-02,  5.4206e-02],\n                        [ 1.9926e-02, -4.6598e-02, -7.3754e-03,  7.9669e-03, -3.3379e-02]],\n              \n                       [[-1.4863e-02, -5.5218e-03,  3.5056e-02,  2.7424e-03, -3.6868e-02],\n                        [-1.0076e-02,  2.9035e-02,  1.8738e-02, -3.4324e-02,  3.3992e-02],\n                        [ 5.1882e-02, -9.1000e-03,  3.8924e-02,  4.0029e-02, -1.5780e-02],\n                        [-2.3913e-02,  2.0820e-02, -2.1381e-02,  1.9137e-02, -2.8265e-02],\n                        [-6.5751e-03, -2.6687e-02, -1.3519e-02, -3.4560e-02, -3.4233e-02]],\n              \n                       ...,\n              \n                       [[ 3.0467e-02,  2.8662e-03, -4.4103e-02, -2.9169e-02, -1.1073e-02],\n                        [-5.2685e-02, -5.7553e-02, -1.5584e-02, -3.9461e-02, -4.9964e-03],\n                        [ 2.0094e-02,  7.1440e-02,  4.9935e-02,  7.0649e-02,  7.0418e-02],\n                        [-3.0211e-02,  4.5912e-02,  6.5314e-02,  1.8362e-02, -1.2120e-02],\n                        [ 4.2749e-02,  1.5629e-02,  2.9329e-02,  3.0158e-02,  4.9687e-02]],\n              \n                       [[ 1.3391e-02,  4.8163e-03, -1.2567e-02,  8.6640e-03,  2.5346e-02],\n                        [ 3.9335e-02, -3.6543e-02, -3.5583e-02,  1.9804e-02, -2.4139e-02],\n                        [ 6.0185e-03,  2.1453e-02, -5.7654e-03, -3.3020e-02, -9.2485e-04],\n                        [-8.5391e-03,  2.0672e-02, -1.7325e-02,  1.0336e-02, -8.0849e-03],\n                        [ 2.5266e-02,  3.8045e-02,  6.4642e-03, -1.7129e-02,  2.9181e-02]],\n              \n                       [[-1.6548e-02, -4.2142e-02, -1.2847e-02, -1.9238e-02, -6.7571e-04],\n                        [-5.5652e-02, -2.4266e-02, -2.1688e-02, -3.1458e-02, -1.6775e-02],\n                        [-5.5382e-02,  2.7552e-02, -1.6708e-02,  6.0198e-03,  3.3075e-02],\n                        [ 3.3595e-02,  7.3604e-02, -2.0137e-02,  2.2669e-02,  3.8452e-02],\n                        [-9.9271e-03, -2.3934e-02,  9.7027e-03,  2.6913e-02, -4.0433e-02]]]])),\n             ('conv2.bias',\n              tensor([ 0.0055,  0.0047,  0.0016,  0.0296,  0.0055,  0.0049, -0.0454,  0.0349,\n                      -0.0323, -0.0346,  0.0200,  0.0510,  0.0142,  0.0359, -0.0183, -0.0409,\n                       0.0047,  0.0374, -0.0448,  0.0051, -0.0135, -0.0470,  0.0158, -0.0043,\n                       0.0441,  0.0394,  0.0032, -0.0083,  0.0399, -0.0399,  0.0380, -0.0374,\n                       0.0319, -0.0155, -0.0066, -0.0114, -0.0176,  0.0391,  0.0077,  0.0427,\n                      -0.0159, -0.0263,  0.0422, -0.0406,  0.0119, -0.0150, -0.0316, -0.0278,\n                       0.0355, -0.0240])),\n             ('fc1.weight',\n              tensor([[-0.0280,  0.0290, -0.0213,  ...,  0.0296, -0.0267,  0.0244],\n                      [-0.0227,  0.0042,  0.0163,  ...,  0.0082, -0.0125, -0.0312],\n                      [ 0.0027, -0.0283,  0.0142,  ...,  0.0175,  0.0302, -0.0215],\n                      ...,\n                      [-0.0310, -0.0002, -0.0109,  ..., -0.0035, -0.0017,  0.0175],\n                      [ 0.0340, -0.0274,  0.0175,  ...,  0.0024, -0.0410,  0.0143],\n                      [-0.0069, -0.0216, -0.0146,  ..., -0.0089, -0.0257,  0.0024]])),\n             ('fc1.bias',\n              tensor([ 3.7823e-03, -6.3193e-03, -2.9323e-02,  3.6288e-02, -1.3190e-02,\n                       2.6011e-02,  3.1792e-02,  1.6356e-02, -2.7479e-02, -2.3399e-02,\n                       6.9019e-03,  2.4311e-02,  1.2253e-02, -1.3124e-02,  3.4076e-02,\n                      -2.5511e-03,  2.7401e-03,  8.1354e-03, -3.0266e-02, -5.0847e-03,\n                      -1.2579e-02, -3.0949e-03, -1.3830e-02, -3.5134e-02,  2.5970e-02,\n                      -3.0589e-02,  1.4873e-03, -6.1178e-03, -3.2684e-02, -3.1645e-02,\n                      -3.4562e-02, -3.5710e-03, -1.4352e-02, -2.4738e-02, -2.5171e-02,\n                       8.4513e-03, -2.6120e-02, -1.4566e-02, -2.2079e-02, -3.3506e-02,\n                      -2.1379e-02,  3.3084e-02, -2.0953e-02,  1.2331e-02,  2.9295e-04,\n                      -1.5636e-02, -1.4889e-02, -8.8701e-03, -2.2740e-02,  3.3181e-02,\n                      -9.5118e-03,  5.3379e-03,  2.6813e-02, -1.9533e-02,  4.9537e-03,\n                      -1.9777e-02,  2.6487e-02,  2.3899e-02, -1.2692e-02, -2.5548e-03,\n                      -2.0652e-02, -5.9276e-03,  1.3175e-02, -2.2000e-02,  2.8915e-02,\n                       1.8139e-02,  2.9127e-02,  2.8481e-05, -3.6358e-04, -2.9898e-02,\n                       1.8397e-02,  2.9716e-02, -1.0064e-02, -3.3025e-02,  1.8036e-02,\n                       3.4948e-02,  3.0127e-02, -2.8920e-02,  1.8992e-03,  2.9426e-02,\n                       2.2913e-02,  1.7796e-02,  7.3823e-03, -2.1826e-02,  2.5233e-02,\n                       2.8930e-02,  4.7418e-03, -2.2560e-02,  1.4477e-02, -1.3318e-02,\n                      -1.0638e-02,  4.2995e-04, -2.4881e-02,  3.3387e-03,  5.1015e-03,\n                      -1.6100e-02, -1.3683e-02,  1.1111e-02,  1.6516e-02, -2.6043e-02,\n                      -4.8945e-03,  2.2844e-02, -1.1314e-02,  3.0601e-02, -3.4601e-03,\n                       2.3378e-02, -2.7619e-03, -1.4724e-02,  2.3237e-03,  2.4400e-02,\n                       3.1040e-03, -1.9870e-02, -2.4960e-02, -2.6151e-02,  3.3641e-02,\n                      -3.9730e-04, -9.1745e-03,  5.8522e-03,  1.0384e-02, -1.8693e-02,\n                      -3.2270e-02, -2.2469e-02, -2.4259e-02, -1.4556e-02,  3.5362e-02,\n                       1.1492e-02, -1.2649e-02,  2.1509e-02, -2.2221e-02,  4.5236e-03,\n                      -2.8863e-02,  7.1119e-03,  3.4711e-02, -1.0558e-02,  2.1783e-02,\n                      -2.0314e-02, -1.1701e-02, -2.1586e-02,  2.3706e-02,  1.8697e-02,\n                      -1.9735e-02, -3.0828e-02, -4.3498e-03, -2.9807e-02, -1.3563e-02,\n                       1.6739e-03,  3.3372e-02,  2.7658e-02, -2.1299e-02,  1.2785e-02,\n                       3.1435e-02, -3.5415e-02,  1.6367e-02,  2.6146e-03,  9.1493e-03,\n                      -1.2811e-02, -5.5691e-03,  1.7538e-02,  2.6848e-02, -3.0582e-02,\n                       2.3787e-02,  9.1214e-03, -2.7548e-02, -2.9058e-02, -2.7874e-02,\n                      -1.0177e-02,  2.1751e-02,  3.1199e-02,  2.6672e-02, -1.9695e-02,\n                      -2.2299e-02, -7.0258e-03, -9.9276e-03,  2.3726e-02,  2.6842e-02,\n                       1.7585e-02,  2.5226e-02,  6.4665e-03, -9.5504e-03,  3.1315e-02,\n                      -1.4538e-03, -1.0204e-02, -1.3046e-02,  1.5754e-03,  1.5724e-02,\n                       1.2769e-02, -1.0923e-02, -2.5415e-02, -1.5109e-02, -8.7835e-03,\n                      -2.7900e-02,  1.5581e-02,  1.0369e-02,  6.9616e-03, -3.0986e-02,\n                      -2.6751e-02,  1.3923e-02, -6.2116e-03,  5.2287e-03,  3.5063e-02,\n                      -1.5829e-02,  2.8092e-02,  1.9513e-02,  3.5311e-03,  4.4219e-03,\n                       2.9914e-02,  1.3939e-03,  2.2235e-02,  2.3611e-02,  6.2519e-03,\n                       8.8657e-03, -2.1520e-02,  1.0987e-02,  1.5384e-02, -2.4625e-02,\n                       2.1070e-02, -4.4969e-03,  3.2403e-02, -1.1541e-02,  1.5936e-03,\n                       3.4810e-02,  3.2529e-02, -2.5954e-02, -2.4166e-02,  1.2924e-03,\n                       9.3856e-03, -5.9848e-03,  2.2633e-02,  1.7084e-02,  5.1496e-05,\n                      -2.6993e-03,  1.7510e-02,  1.9485e-02,  2.3107e-02,  1.8893e-02,\n                       1.6773e-02,  3.5128e-02, -2.9836e-02, -1.3570e-02,  3.2982e-02,\n                      -1.5413e-02, -3.1185e-02,  1.6206e-02, -3.3778e-02,  3.0039e-02,\n                       1.3080e-03, -2.5667e-03,  1.0596e-02, -5.9106e-03, -2.7133e-02,\n                       1.2578e-02, -2.7397e-02,  6.2333e-03,  3.3784e-02,  7.6015e-04,\n                       5.3316e-03, -1.6062e-02,  4.7879e-03, -1.3576e-02, -1.9749e-02,\n                       1.4968e-03,  9.2922e-03, -2.6113e-02, -1.8036e-02,  1.2330e-02,\n                      -2.4442e-02,  2.9662e-02, -1.6899e-02,  2.3484e-02,  2.0866e-02,\n                       6.0778e-03,  2.0996e-02,  1.7130e-02,  1.5170e-02, -1.1958e-02,\n                      -2.3889e-02,  2.2931e-02, -1.8424e-02,  3.4738e-03, -3.1783e-02,\n                      -1.8984e-02, -1.4259e-03, -1.0048e-02, -2.0015e-02, -1.2255e-02,\n                       1.6815e-02, -2.5100e-02,  1.1245e-02,  2.2590e-02, -2.5238e-02,\n                       9.8161e-03, -1.5749e-02,  1.5014e-02,  1.6909e-02,  2.7401e-02,\n                      -6.1200e-03,  1.3814e-02, -2.1814e-02,  3.0023e-02, -3.0260e-02,\n                      -1.8569e-02,  2.8626e-02, -4.3810e-03, -1.1301e-02, -2.9149e-02,\n                       2.1685e-02, -1.6979e-02,  1.1027e-02, -2.9893e-02, -8.3117e-03,\n                      -1.2092e-02,  3.0804e-02,  3.2664e-02,  7.9962e-03, -3.0537e-02,\n                      -1.7236e-02, -1.2290e-02,  8.5065e-03, -3.4512e-02,  1.9316e-02,\n                      -2.5552e-02,  1.7906e-02,  5.2845e-03, -5.1819e-03, -1.9205e-02,\n                      -5.8872e-04, -1.9768e-02,  3.7822e-03, -3.2114e-02,  6.2503e-04,\n                       3.4032e-02, -1.5664e-02,  3.1724e-02, -9.9310e-03,  2.3103e-02,\n                       1.0610e-02,  8.6337e-03, -2.1450e-02,  1.7125e-02, -3.2250e-02,\n                       4.9308e-03, -1.2664e-02,  1.6949e-02, -2.4074e-02,  9.4959e-03,\n                      -1.8452e-02, -1.9511e-02,  1.4484e-02, -9.7801e-03,  2.0046e-02,\n                       3.2135e-03, -3.2048e-02,  1.6769e-03, -2.5714e-02,  2.5071e-02,\n                       3.7269e-03, -1.4658e-03,  1.5914e-02,  2.3270e-02,  2.5933e-02,\n                       1.8365e-02,  2.9366e-02, -2.0051e-02,  3.7713e-03, -2.3358e-02,\n                       2.3973e-02,  1.7476e-02, -2.7065e-02,  3.9604e-03,  1.0309e-02,\n                       3.2501e-02, -2.8927e-02,  4.5870e-03,  2.7069e-02,  4.1799e-03,\n                       5.3039e-03, -2.6632e-02,  9.5724e-03,  3.6277e-02,  2.9953e-02,\n                       1.9907e-02,  1.3101e-02,  7.1791e-03, -1.8430e-02, -8.9621e-04,\n                       2.9643e-02, -2.8800e-02,  1.5894e-02,  2.3382e-02, -1.1447e-02,\n                      -8.7590e-03, -2.1962e-02, -2.0812e-04, -4.6937e-03,  3.2189e-02,\n                       3.7955e-03, -5.8172e-03,  3.3448e-02, -2.4410e-02,  1.2794e-02,\n                      -3.1947e-02, -1.1594e-02,  2.7599e-02, -1.7238e-02,  1.6989e-02,\n                      -7.2571e-03,  6.6572e-03,  1.9548e-02, -8.0865e-03,  2.7309e-02,\n                       2.5641e-04, -1.2806e-02, -2.8045e-02,  8.0228e-03,  3.0545e-02,\n                       2.3545e-02,  2.8470e-02,  9.0352e-04,  2.3189e-02, -2.0846e-03,\n                       2.0832e-02,  2.9482e-02,  3.4712e-02,  1.6625e-03, -3.1291e-02,\n                       2.5671e-02,  2.8683e-02,  1.5155e-02, -2.8153e-02,  3.3286e-02,\n                       2.7881e-02, -7.2207e-03,  1.4314e-02, -8.3424e-03,  6.4896e-03,\n                      -3.0340e-02, -1.5304e-03, -3.0700e-02, -1.3676e-03,  6.6331e-03,\n                       3.3812e-03,  3.6338e-02, -1.9248e-02,  5.9030e-03,  2.1797e-02,\n                       1.5215e-03, -8.8929e-03, -4.9120e-03, -2.4449e-02, -3.4068e-03,\n                      -6.6480e-03, -1.8693e-03, -7.5528e-03, -3.3355e-02,  2.9662e-02,\n                      -2.5428e-02, -2.8663e-02,  9.5430e-03,  1.0065e-02,  3.2680e-02,\n                       2.5688e-02,  2.7435e-02,  1.2460e-02, -3.3442e-02,  7.7164e-03,\n                       8.5751e-03,  3.0993e-02,  2.2921e-02,  2.3426e-02, -2.5895e-04,\n                       8.9923e-05,  2.7571e-02,  3.3977e-02, -9.4997e-04, -2.8186e-02,\n                       1.3488e-02, -3.0036e-02, -4.8562e-04, -1.0154e-03, -1.7540e-02,\n                      -8.6032e-03,  2.9524e-02, -2.8222e-02,  3.1373e-03,  1.7040e-02,\n                       2.3419e-02,  1.0826e-02, -3.8976e-04,  7.8252e-03,  2.5772e-02,\n                      -1.0385e-02,  1.2500e-02,  3.1345e-02,  5.8408e-03, -1.0830e-02,\n                       3.2314e-02, -1.8152e-03, -1.7246e-02, -1.6589e-02, -2.3855e-02])),\n             ('fc2.weight',\n              tensor([[-0.0599,  0.0430, -0.0062,  ...,  0.0053, -0.0249, -0.0323],\n                      [-0.0365, -0.0300,  0.0686,  ...,  0.0615,  0.1339, -0.0157],\n                      [ 0.0356, -0.0286, -0.0449,  ..., -0.0247, -0.0434, -0.0560],\n                      ...,\n                      [-0.0510,  0.1094,  0.0122,  ..., -0.0839,  0.0224,  0.0385],\n                      [-0.0303, -0.0719,  0.0249,  ...,  0.0225, -0.1159, -0.0123],\n                      [ 0.0417, -0.0729,  0.0132,  ...,  0.0012, -0.0230, -0.0089]])),\n             ('fc2.bias',\n              tensor([-0.0038, -0.0255, -0.0165, -0.0412, -0.0472,  0.0064,  0.0273, -0.0355,\n                      -0.0298,  0.0067]))])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 70
    }
   ],
   "source": [
    "model.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "pycharm-e85a843b",
   "language": "python",
   "display_name": "PyCharm (ml_study)"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}